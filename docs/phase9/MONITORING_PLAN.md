# Phase 9: Staging Canary Monitoring Plan

**Document ID:** PHASE9-MONITORING-001
**Version:** 1.0.0
**Date:** 2025-10-06
**Status:** üü¢ ACTIVE
**Owner:** Analyst Agent (Hive Mind Swarm)

---

## Executive Summary

This document defines comprehensive monitoring, metrics, alerting, and evidence collection for the Phase 8 staging canary deployment. It establishes SLO thresholds, rollback triggers, and dashboard specifications to ensure safe, observable progressive rollout.

**Mission:** Enable data-driven canary decisions through real-time monitoring and automated evidence collection.

---

## Table of Contents

1. [Metrics Catalog](#1-metrics-catalog)
2. [Dashboard Specifications](#2-dashboard-specifications)
3. [Alert Thresholds](#3-alert-thresholds)
4. [Rollback Triggers](#4-rollback-triggers)
5. [Evidence Collection](#5-evidence-collection)
6. [SLO Tracking](#6-slo-tracking)
7. [User Journey Funnels](#7-user-journey-funnels)
8. [Automation & Tooling](#8-automation--tooling)

---

## 1. Metrics Catalog

### 1.1 Performance Metrics

#### 1.1.1 Latency Metrics

| Metric | Baseline | Canary Target | Threshold | Unit | Collection Method |
|--------|----------|---------------|-----------|------|-------------------|
| **P50 Latency** | 145ms | ‚â§160ms | ‚â§200ms | milliseconds | Nginx access logs (request_time) |
| **P95 Latency** | 385ms | ‚â§420ms | ‚â§500ms | milliseconds | Nginx access logs (request_time) |
| **P99 Latency** | 720ms | ‚â§800ms | ‚â§1000ms | milliseconds | Nginx access logs (request_time) |
| **API Response Time** | 180ms | ‚â§200ms | ‚â§300ms | milliseconds | Laravel telescope/metrics |
| **Database Query Time** | 35ms | ‚â§45ms | ‚â§100ms | milliseconds | Laravel query log |
| **Encryption Overhead** | N/A | ‚â§8ms | ‚â§15ms | milliseconds | Custom timer in EncryptsAttributes |
| **Analytics Write Time** | N/A | ‚â§15ms | ‚â§30ms | milliseconds | AnalyticsEventRepository timer |

**Collection Script:**
```bash
#!/bin/bash
# Extract P50/P95/P99 latency from Nginx logs
awk '{print $NF}' /var/log/nginx/access.log | \
  sort -n | \
  awk 'BEGIN {c=0} {v[c++]=$1} END {
    print "P50:", v[int(c*0.50)], "ms";
    print "P95:", v[int(c*0.95)], "ms";
    print "P99:", v[int(c*0.99)], "ms";
  }'
```

#### 1.1.2 Throughput Metrics

| Metric | Baseline | Canary Target | Threshold | Unit | Collection Method |
|--------|----------|---------------|-----------|------|-------------------|
| **Requests/Second** | 125/s | ‚â•110/s | ‚â•50/s | req/s | Nginx status module |
| **Document Uploads/Min** | N/A | ‚â•5/min | ‚â•2/min | uploads/min | Application metrics |
| **Auth Requests/Min** | 45/min | ‚â•40/min | ‚â•20/min | req/min | Sanctum token count |
| **Gamification Events/Min** | 120/min | ‚â•100/min | ‚â•50/min | events/min | GamificationController metrics |

**Collection Query:**
```sql
-- Throughput from analytics_events table
SELECT
  COUNT(*) / 60 as events_per_minute,
  event_name,
  DATE_FORMAT(created_at, '%Y-%m-%d %H:%i:00') as minute_bucket
FROM analytics_events
WHERE created_at >= NOW() - INTERVAL 5 MINUTE
GROUP BY event_name, minute_bucket
ORDER BY minute_bucket DESC;
```

#### 1.1.3 Error Rate Metrics

| Metric | Baseline | Canary Target | Threshold | Unit | Collection Method |
|--------|----------|---------------|-----------|------|-------------------|
| **Overall Error Rate** | 0.28% | ‚â§0.40% | ‚â§1.0% | percentage | HTTP 5xx count / total requests |
| **4xx Error Rate** | 2.1% | ‚â§2.5% | ‚â§5.0% | percentage | HTTP 4xx count / total requests |
| **5xx Error Rate** | 0.28% | ‚â§0.40% | ‚â§1.0% | percentage | HTTP 5xx count / total requests |
| **Database Errors** | 0.05% | ‚â§0.10% | ‚â§0.5% | percentage | Laravel exception log |
| **Validation Errors** | 1.8% | ‚â§2.0% | ‚â§4.0% | percentage | HTTP 422 count / total requests |

**Collection Script:**
```bash
#!/bin/bash
# Calculate error rates from Nginx logs
total=$(wc -l < /var/log/nginx/access.log)
errors_4xx=$(grep ' 4[0-9][0-9] ' /var/log/nginx/access.log | wc -l)
errors_5xx=$(grep ' 5[0-9][0-9] ' /var/log/nginx/access.log | wc -l)

echo "4xx Rate: $(awk "BEGIN {printf \"%.2f%%\", ($errors_4xx/$total)*100}")"
echo "5xx Rate: $(awk "BEGIN {printf \"%.2f%%\", ($errors_5xx/$total)*100}")"
```

### 1.2 Analytics Metrics

#### 1.2.1 Event Ingestion

| Metric | Baseline | Canary Target | Threshold | Unit | Collection Method |
|--------|----------|---------------|-----------|------|-------------------|
| **Event Ingestion Success** | N/A | ‚â•99.5% | ‚â•95.0% | percentage | Success / total persist attempts |
| **Schema Validation Failures** | N/A | ‚â§0.5% | ‚â§2.0% | percentage | Invalid schema / total events |
| **PII Detection Count** | 0 | 0 | 0 | count | PII regex matches (MUST be zero) |
| **Analytics Write Latency** | N/A | ‚â§15ms | ‚â§30ms | milliseconds | AnalyticsEventRepository timer |
| **Event Queue Depth** | 0 | ‚â§100 | ‚â§500 | events | Queue length before persistence |

**Validation Query:**
```sql
-- Analytics health check
SELECT
  COUNT(*) as total_events,
  SUM(CASE WHEN event_payload IS NULL THEN 1 ELSE 0 END) as null_payloads,
  SUM(CASE WHEN user_id_hash IS NULL THEN 1 ELSE 0 END) as null_user_hashes,
  COUNT(DISTINCT event_name) as unique_event_types,
  MIN(created_at) as oldest_event,
  MAX(created_at) as newest_event
FROM analytics_events
WHERE created_at >= NOW() - INTERVAL 1 HOUR;
```

#### 1.2.2 Event Type Distribution

| Event Name | Expected Rate | Alert Threshold | Business Impact |
|------------|---------------|-----------------|------------------|
| `gamification.points_earned` | 80-120/min | <50/min OR >200/min | Gamification broken |
| `auth.user_registered` | 5-15/min | <2/min OR >30/min | Registration funnel broken |
| `auth.email_verified` | 4-12/min | <2/min OR >25/min | Email verification broken |
| `auth.user_logged_in` | 20-40/min | <10/min OR >100/min | Auth system degraded |
| `documents.presigned_url_generated` | 5-10/min | <2/min OR >20/min | Document upload broken |
| `documents.upload_complete` | 4-9/min | <2/min OR >18/min | S3 upload failures |

### 1.3 Security Metrics

#### 1.3.1 Encryption Health

| Metric | Baseline | Canary Target | Threshold | Unit | Collection Method |
|--------|----------|---------------|-----------|------|-------------------|
| **PHI Encryption Rate** | 100% | 100% | 100% | percentage | Encrypted fields / total PHI fields |
| **Encryption Errors** | 0 | 0 | 0 | count | Crypt::decrypt exceptions |
| **Hash Generation Failures** | 0 | 0 | 0 | count | SHA256 hash errors |
| **Plaintext PHI Detections** | 0 | 0 | 0 | count | Regex scans of DB dumps |
| **Key Rotation Status** | Fresh | Fresh | ‚â§30 days | days | APP_KEY last rotation date |

**Verification Script:**
```php
<?php
// Check encryption health
use App\Models\User;
use Illuminate\Support\Facades\Crypt;

$users = User::limit(100)->get();
$encrypted_count = 0;
$plaintext_count = 0;

foreach ($users as $user) {
    try {
        // Attempt decryption (should succeed)
        $decrypted_cpf = $user->cpf;
        $encrypted_count++;

        // Verify hash column exists
        if (empty($user->cpf_hash)) {
            throw new \Exception("Missing cpf_hash for user {$user->id}");
        }
    } catch (\Exception $e) {
        $plaintext_count++;
        Log::error("Encryption check failed", ['user_id' => $user->id, 'error' => $e->getMessage()]);
    }
}

echo "Encryption Rate: " . (($encrypted_count / 100) * 100) . "%\n";
echo "Failures: {$plaintext_count}\n";
```

#### 1.3.2 Database Security

| Metric | Baseline | Canary Target | Threshold | Unit | Collection Method |
|--------|----------|---------------|-----------|------|-------------------|
| **TLS Connection Rate** | 100% | 100% | 100% | percentage | SSL cipher != null |
| **TLS Version Compliance** | TLS 1.2+ | TLS 1.2+ | TLS 1.2+ | version | SHOW STATUS LIKE 'Ssl_version' |
| **Failed Auth Attempts** | <5/min | <8/min | <20/min | attempts/min | Laravel auth log |
| **SQL Injection Attempts** | 0 | 0 | 0 | count | WAF/IDS alerts |
| **CSRF Token Validation Rate** | 100% | 100% | 100% | percentage | Valid tokens / total requests |

**TLS Verification:**
```sql
-- Verify TLS connection
SHOW STATUS LIKE 'Ssl_cipher';
-- Expected: TLS_AES_256_GCM_SHA384 or similar

SHOW STATUS LIKE 'Ssl_version';
-- Expected: TLSv1.2 or TLSv1.3
```

### 1.4 Reliability Metrics

#### 1.4.1 Queue Health

| Metric | Baseline | Canary Target | Threshold | Unit | Collection Method |
|--------|----------|---------------|-----------|------|-------------------|
| **Queue Lag** | 0s | ‚â§2s | ‚â§10s | seconds | Current time - oldest job timestamp |
| **Failed Jobs** | 0/hr | ‚â§5/hr | ‚â§20/hr | jobs/hour | Laravel failed_jobs table count |
| **Job Retry Rate** | 2% | ‚â§5% | ‚â§15% | percentage | Retried jobs / total jobs |
| **Analytics Persist Queue** | 0 | ‚â§50 | ‚â§200 | events | Pending analytics writes |

**Queue Monitoring:**
```sql
-- Check queue health
SELECT
  queue,
  COUNT(*) as pending_jobs,
  MIN(available_at) as oldest_job_timestamp,
  TIMESTAMPDIFF(SECOND, MIN(available_at), NOW()) as queue_lag_seconds
FROM jobs
GROUP BY queue;

-- Check failed jobs
SELECT
  queue,
  COUNT(*) as failed_count,
  MAX(failed_at) as last_failure
FROM failed_jobs
WHERE failed_at >= NOW() - INTERVAL 1 HOUR
GROUP BY queue;
```

#### 1.4.2 Storage Health

| Metric | Baseline | Canary Target | Threshold | Unit | Collection Method |
|--------|----------|---------------|-----------|------|-------------------|
| **S3 Upload Success Rate** | N/A | ‚â•99.0% | ‚â•95.0% | percentage | Successful uploads / total attempts |
| **S3 4xx Error Rate** | N/A | ‚â§0.5% | ‚â§2.0% | percentage | 4xx errors / total requests |
| **S3 5xx Error Rate** | N/A | ‚â§0.1% | ‚â§1.0% | percentage | 5xx errors / total requests |
| **Presigned URL Expiry** | 5min | 5min | 5min | minutes | URL TTL configuration |
| **Storage Growth Rate** | N/A | 5-10GB/day | >20GB/day | GB/day | S3 bucket size delta |

### 1.5 Resource Utilization

#### 1.5.1 Infrastructure Metrics

| Metric | Baseline | Canary Target | Threshold | Unit | Collection Method |
|--------|----------|---------------|-----------|------|-------------------|
| **CPU Utilization** | 45% | ‚â§60% | ‚â§85% | percentage | CloudWatch/top |
| **Memory Usage** | 52% | ‚â§65% | ‚â§85% | percentage | CloudWatch/free -m |
| **Database Connections** | 45/100 | ‚â§70/100 | ‚â§90/100 | connections | SHOW STATUS LIKE 'Threads_connected' |
| **Cache Hit Rate** | 92% | ‚â•85% | ‚â•70% | percentage | Redis INFO stats |
| **Disk I/O** | 15MB/s | ‚â§25MB/s | ‚â§50MB/s | MB/s | iostat |

**Resource Monitoring:**
```bash
#!/bin/bash
# Resource utilization snapshot
echo "=== CPU ==="
top -bn1 | grep "Cpu(s)" | awk '{print "Usage: " 100 - $8 "%"}'

echo "=== Memory ==="
free -m | awk 'NR==2{printf "Used: %.2f%% (%dMB / %dMB)\n", $3*100/$2, $3, $2}'

echo "=== Database Connections ==="
mysql -e "SHOW STATUS LIKE 'Threads_connected';" | awk 'NR==2{print "Active: " $2}'

echo "=== Cache Hit Rate ==="
redis-cli INFO stats | grep keyspace_hits | awk -F: '{print "Hits: " $2}'
redis-cli INFO stats | grep keyspace_misses | awk -F: '{print "Misses: " $2}'
```

---

## 2. Dashboard Specifications

### 2.1 Real-Time Canary Dashboard

**Purpose:** Live monitoring during canary progression
**Update Interval:** 30 seconds
**Technology:** Grafana with Prometheus data source

#### Layout Specification

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                   PHASE 8 CANARY DEPLOYMENT                          ‚ïë
‚ïë                      Staging Environment                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CANARY STATUS                                          Last Update: 30s‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ Stage: 2/4 (25% Traffic)                                    ‚úÖ HEALTHY ‚îÇ
‚îÇ Progress: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 85/120 min                 ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ Traffic Split:                                                          ‚îÇ
‚îÇ   Production (75%): ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë              ‚îÇ
‚îÇ   Canary (25%):     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PERFORMANCE METRICS                                                    ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ Metric          ‚îÇ Baseline  ‚îÇ Current   ‚îÇ Threshold ‚îÇ Status          ‚îÇ
‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ P50 Latency     ‚îÇ 145ms     ‚îÇ 152ms ‚úì   ‚îÇ 200ms     ‚îÇ ‚úÖ 76% margin   ‚îÇ
‚îÇ P95 Latency     ‚îÇ 385ms     ‚îÇ 402ms ‚úì   ‚îÇ 500ms     ‚îÇ ‚úÖ 80% margin   ‚îÇ
‚îÇ P99 Latency     ‚îÇ 720ms     ‚îÇ 758ms ‚úì   ‚îÇ 1000ms    ‚îÇ ‚úÖ 76% margin   ‚îÇ
‚îÇ Error Rate      ‚îÇ 0.28%     ‚îÇ 0.35% ‚úì   ‚îÇ 1.0%      ‚îÇ ‚úÖ 65% margin   ‚îÇ
‚îÇ Throughput      ‚îÇ 125/s     ‚îÇ 126/s ‚úì   ‚îÇ >50/s     ‚îÇ ‚úÖ PASS         ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ SLO Compliance: 5/5 (100%) ‚úÖ                                          ‚îÇ
‚îÇ Consecutive Breaches: 0/3                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ANALYTICS HEALTH                                                       ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ Event Ingestion:     ‚úÖ 127 events/min (target: >100/min)             ‚îÇ
‚îÇ Schema Validation:   ‚úÖ 99.8% success (target: >99%)                  ‚îÇ
‚îÇ PII Detections:      ‚úÖ 0 (MUST BE ZERO)                              ‚îÇ
‚îÇ Write Latency:       ‚úÖ 14.2ms (threshold: <30ms)                     ‚îÇ
‚îÇ Queue Depth:         ‚úÖ 23 events (threshold: <500)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SECURITY STATUS                                                        ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ Encryption:          ‚úÖ 100% PHI encrypted                            ‚îÇ
‚îÇ Database TLS:        ‚úÖ TLSv1.3 (cipher: TLS_AES_256_GCM_SHA384)      ‚îÇ
‚îÇ Failed Auth:         ‚úÖ 3/min (threshold: <20/min)                    ‚îÇ
‚îÇ CSRF Validation:     ‚úÖ 100% (no rejections)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ RESOURCE UTILIZATION                                                   ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ CPU:                 ‚úÖ 47% (threshold: <85%)                         ‚îÇ
‚îÇ Memory:              ‚úÖ 58% (threshold: <85%)                         ‚îÇ
‚îÇ DB Connections:      ‚úÖ 52/100 (threshold: <90)                       ‚îÇ
‚îÇ Cache Hit Rate:      ‚úÖ 91.3% (threshold: >70%)                       ‚îÇ
‚îÇ Disk I/O:            ‚úÖ 18MB/s (threshold: <50MB/s)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ USER JOURNEY FUNNEL (Last 5 min)                                      ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ Registration Started:     45 users                                     ‚îÇ
‚îÇ   ‚îî‚îÄ‚ñ∫ Email Verified:     41 users (91.1%) ‚úÖ                         ‚îÇ
‚îÇ       ‚îî‚îÄ‚ñ∫ Profile Complete: 38 users (92.7%) ‚úÖ                       ‚îÇ
‚îÇ           ‚îî‚îÄ‚ñ∫ Document Upload: 32 users (84.2%) ‚ö†Ô∏è                    ‚îÇ
‚îÇ               ‚îî‚îÄ‚ñ∫ Approved:    28 users (87.5%) ‚úÖ                    ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ Overall Conversion: 62.2% (started ‚Üí approved)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ RECENT ALERTS (Last 15 min)                                           ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ [18:42:15] ‚ÑπÔ∏è  INFO: Canary traffic increased to 25%                 ‚îÇ
‚îÇ [18:38:22] ‚úÖ OK: P95 latency returned to normal (398ms)              ‚îÇ
‚îÇ [18:35:10] ‚ö†Ô∏è  WARN: P95 latency spike detected (512ms) - recovered  ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ No critical alerts in last 15 minutes ‚úÖ                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

[Auto-Refresh: 30s] [Export CSV] [View Raw Metrics] [Trigger Rollback]
```

#### Grafana Panel Configuration

**Panel 1: Latency Time Series**
```json
{
  "title": "Latency Distribution (P50/P95/P99)",
  "type": "graph",
  "datasource": "Prometheus",
  "targets": [
    {
      "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
      "legendFormat": "P50"
    },
    {
      "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
      "legendFormat": "P95"
    },
    {
      "expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))",
      "legendFormat": "P99"
    }
  ],
  "yAxis": {
    "format": "ms",
    "min": 0
  },
  "alert": {
    "conditions": [
      {
        "evaluator": { "type": "gt", "params": [500] },
        "operator": { "type": "and" },
        "query": { "params": ["P95", "5m", "now"] },
        "reducer": { "type": "avg" }
      }
    ]
  }
}
```

**Panel 2: Error Rate**
```json
{
  "title": "Error Rate (4xx / 5xx)",
  "type": "graph",
  "datasource": "Prometheus",
  "targets": [
    {
      "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m])) * 100",
      "legendFormat": "5xx Error Rate"
    },
    {
      "expr": "sum(rate(http_requests_total{status=~\"4..\"}[5m])) / sum(rate(http_requests_total[5m])) * 100",
      "legendFormat": "4xx Error Rate"
    }
  ],
  "yAxis": {
    "format": "percent",
    "min": 0,
    "max": 5
  }
}
```

**Panel 3: Analytics Event Rate**
```json
{
  "title": "Analytics Events/Minute",
  "type": "graph",
  "datasource": "MySQL",
  "targets": [
    {
      "rawSql": "SELECT created_at AS time, COUNT(*)/60 AS value FROM analytics_events WHERE created_at >= NOW() - INTERVAL 30 MINUTE GROUP BY DATE_FORMAT(created_at, '%Y-%m-%d %H:%i:00')"
    }
  ],
  "yAxis": {
    "format": "short",
    "min": 0
  }
}
```

### 2.2 Historical Trends Dashboard

**Purpose:** Post-deployment analysis and trend identification
**Update Interval:** 5 minutes
**Time Range:** Last 7 days

**Panels:**
1. **Latency Trends**: 7-day P50/P95/P99 trend lines with moving average
2. **Error Rate Heatmap**: Hourly error rate distribution (identify peak hours)
3. **Feature Adoption**: Phase 8 feature usage over time
4. **Resource Utilization**: CPU/Memory/DB connections trend
5. **Cost Analysis**: Infrastructure cost breakdown (compute, storage, egress)

### 2.3 Compliance Dashboard

**Purpose:** Regulatory compliance monitoring
**Update Interval:** 1 hour
**Retention:** 2 years (audit requirement)

**Panels:**
1. **Encryption Status**: PHI encryption rate (MUST be 100%)
2. **PII Leakage Detector**: Zero PII in analytics (MUST be 0)
3. **TLS Compliance**: Database TLS connection rate (MUST be 100%)
4. **Audit Log Coverage**: WHO-WHAT-WHEN-WHERE-HOW completeness
5. **Data Retention**: 90-day analytics retention compliance

---

## 3. Alert Thresholds

### 3.1 Critical Alerts (Immediate Response)

**Trigger:** Auto-rollback OR page on-call engineer

| Alert Name | Condition | Threshold | Breach Count | Action |
|------------|-----------|-----------|--------------|--------|
| **P95 Latency Breach** | P95 > 500ms | 3 consecutive samples (90s) | 3/3 | üö® AUTO-ROLLBACK |
| **Error Rate Spike** | 5xx rate > 1.0% | 3 consecutive samples (90s) | 3/3 | üö® AUTO-ROLLBACK |
| **Database Down** | DB connection failure | 1 failure | 1/1 | üö® AUTO-ROLLBACK + PAGE |
| **PII Leakage Detected** | PII in analytics | 1 occurrence | 1/1 | üö® AUTO-ROLLBACK + PAGE |
| **Encryption Failure** | PHI encryption < 100% | 1 occurrence | 1/1 | üö® AUTO-ROLLBACK + PAGE |
| **TLS Connection Lost** | DB TLS = null | 1 occurrence | 1/1 | üö® AUTO-ROLLBACK + PAGE |

**Notification Channels:**
- PagerDuty: Immediate page to on-call engineer
- Slack: `#prod-alerts` channel
- Email: devops@company.com, security@company.com
- SMS: On-call rotation

**Auto-Rollback Procedure:**
1. Stop canary traffic routing (0% canary)
2. Clear application cache
3. Reload Laravel config
4. Verify baseline metrics restored
5. Notify engineering team
6. Generate incident report

### 3.2 High-Priority Alerts (15-minute response)

**Trigger:** Notify on-call team, manual investigation required

| Alert Name | Condition | Threshold | Duration | Action |
|------------|-----------|-----------|----------|--------|
| **P99 Latency Elevated** | P99 > 1000ms | Sustained 5min | 5min | üü† NOTIFY + INVESTIGATE |
| **Throughput Drop** | Requests < 50/s | Sustained 5min | 5min | üü† NOTIFY + INVESTIGATE |
| **Analytics Ingestion Failure** | Success rate < 95% | Sustained 5min | 5min | üü† NOTIFY + INVESTIGATE |
| **Queue Lag** | Lag > 10s | Sustained 5min | 5min | üü† NOTIFY + INVESTIGATE |
| **Failed Jobs Spike** | Failed jobs > 20/hr | Sustained 1hr | 1hr | üü† NOTIFY + INVESTIGATE |
| **CPU/Memory High** | Utilization > 85% | Sustained 10min | 10min | üü† NOTIFY + INVESTIGATE |

**Notification Channels:**
- Slack: `#canary-alerts` channel
- Email: engineering-team@company.com
- Dashboard: Mark alert in Grafana

### 3.3 Medium-Priority Alerts (1-hour response)

**Trigger:** Log alert, notify during business hours

| Alert Name | Condition | Threshold | Duration | Action |
|------------|-----------|-----------|----------|--------|
| **4xx Error Rate Elevated** | 4xx rate > 5% | Sustained 15min | 15min | üü° LOG + NOTIFY |
| **Cache Hit Rate Low** | Hit rate < 70% | Sustained 30min | 30min | üü° LOG + NOTIFY |
| **Storage Growth High** | Growth > 20GB/day | Daily check | 1 day | üü° LOG + NOTIFY |
| **Feature Adoption Low** | <10% user engagement | Daily check | 1 day | üü° LOG + NOTIFY |
| **Schema Validation Failures** | Failures > 2% | Sustained 1hr | 1hr | üü° LOG + NOTIFY |

**Notification Channels:**
- Slack: `#engineering` channel
- Daily digest email

### 3.4 Low-Priority Alerts (Informational)

**Trigger:** Log only, review in weekly retrospective

| Alert Name | Condition | Action |
|------------|-----------|--------|
| **Disk I/O Elevated** | I/O > 50MB/s | ‚ÑπÔ∏è LOG |
| **Connection Pool Usage** | Connections > 70/100 | ‚ÑπÔ∏è LOG |
| **Encryption Overhead** | Overhead > 15ms | ‚ÑπÔ∏è LOG |
| **Job Retry Rate** | Retries > 15% | ‚ÑπÔ∏è LOG |

---

## 4. Rollback Triggers

### 4.1 Automatic Rollback Conditions

**Trigger Logic:** ANY of the following conditions met for 3 consecutive samples (90 seconds)

```python
# Rollback decision algorithm
def should_rollback(metrics):
    """
    Evaluate if automatic rollback should be triggered.

    Returns: (should_rollback: bool, reason: str)
    """
    # Critical SLO breaches
    if metrics['p95_latency'] > 500:  # ms
        return (True, "P95 latency exceeds 500ms threshold")

    if metrics['error_rate'] > 1.0:  # percentage
        return (True, "Error rate exceeds 1.0% threshold")

    if metrics['throughput'] < 50:  # req/s
        return (True, "Throughput below 50 req/s minimum")

    # Security violations
    if metrics['pii_detections'] > 0:
        return (True, "PII leakage detected in analytics")

    if metrics['phi_encryption_rate'] < 100:
        return (True, "PHI encryption rate below 100%")

    if metrics['db_tls_enabled'] == False:
        return (True, "Database TLS connection lost")

    # Infrastructure failures
    if metrics['db_connection_failed']:
        return (True, "Database connection failure")

    if metrics['cache_unavailable']:
        return (True, "Redis cache unavailable")

    # No rollback needed
    return (False, None)

# Consecutive breach tracking
breach_count = 0
max_breaches = 3

for sample in monitoring_samples:
    metrics = collect_metrics()
    should_rollback, reason = should_rollback(metrics)

    if should_rollback:
        breach_count += 1
        log_alert(f"SLO breach detected ({breach_count}/{max_breaches}): {reason}")

        if breach_count >= max_breaches:
            trigger_auto_rollback(reason)
            send_pagerduty_alert(reason)
            break
    else:
        # Reset breach counter if SLOs recovered
        if breach_count > 0:
            log_info(f"SLO breach recovered. Resetting counter from {breach_count} to 0.")
        breach_count = 0
```

### 4.2 Manual Rollback Triggers

**Human Decision Required:**

1. **Business Impact**
   - User complaints > 10 tickets/hour
   - Revenue impact > $1000/hour
   - Partner escalations

2. **Operational Concerns**
   - On-call engineer gut feeling
   - Unexpected behavior patterns
   - Pre-scheduled maintenance window conflict

3. **Security Concerns**
   - Suspicious activity patterns
   - Unidentified anomalies
   - Security team recommendation

**Manual Rollback Process:**
```bash
# Trigger manual rollback
cd /var/www/omni-portal/backend
sudo -u www-data ./scripts/rollback-canary.sh \
  --reason "manual-security-concern" \
  --initiated-by "john.doe@company.com"

# Verify rollback success
php artisan db:verify-tls
php artisan health:check
php artisan cache:clear

# Generate incident report
./scripts/generate-incident-report.sh \
  --start-time "2025-10-06 18:30:00" \
  --end-time "2025-10-06 18:45:00" \
  --output /tmp/incident-report.md
```

### 4.3 Rollback Verification

**Post-Rollback Checklist:**

- [ ] Canary traffic reduced to 0%
- [ ] Feature flags disabled (phase8_* = false)
- [ ] Database migrations rolled back (if needed)
- [ ] Application cache cleared
- [ ] Laravel config reloaded
- [ ] P50/P95/P99 latency restored to baseline (¬±10%)
- [ ] Error rate restored to baseline (¬±0.1%)
- [ ] Throughput restored to baseline (¬±5%)
- [ ] Database TLS verified (SHOW STATUS LIKE 'Ssl_cipher')
- [ ] PHI encryption verified (100%)
- [ ] Analytics ingestion working (success rate >99%)
- [ ] Incident report generated
- [ ] Stakeholders notified

**Verification Script:**
```bash
#!/bin/bash
# Post-rollback verification

echo "=== Rollback Verification ==="

# Check canary percentage
canary_pct=$(curl -s localhost/api/v1/feature-flags | jq -r '.canary_rollout_percentage')
echo "Canary Traffic: ${canary_pct}% (expected: 0%)"

# Check latency
p95=$(awk '{print $NF}' /var/log/nginx/access.log | tail -1000 | sort -n | awk 'NR==950{print}')
echo "P95 Latency: ${p95}ms (baseline: 385ms)"

# Check error rate
errors=$(grep ' 5[0-9][0-9] ' /var/log/nginx/access.log | tail -1000 | wc -l)
echo "Error Rate: $(awk "BEGIN {printf \"%.2f%%\", ($errors/1000)*100}")"

# Check database TLS
db_tls=$(mysql -e "SHOW STATUS LIKE 'Ssl_cipher'" | awk 'NR==2{print $2}')
echo "Database TLS: ${db_tls}"

if [ "${canary_pct}" -eq 0 ] && [ "${db_tls}" != "" ]; then
    echo "‚úÖ Rollback verification PASSED"
    exit 0
else
    echo "‚ùå Rollback verification FAILED"
    exit 1
fi
```

---

## 5. Evidence Collection

### 5.1 Evidence Templates

#### 5.1.1 Pre-Canary Baseline Evidence

**File:** `evidence/baseline-YYYYMMDD-HHMMSS.json`

```json
{
  "evidence_type": "baseline",
  "timestamp": "2025-10-06T18:00:00Z",
  "environment": "staging",
  "canary_stage": "pre-deployment",
  "metrics": {
    "performance": {
      "p50_latency_ms": 145,
      "p95_latency_ms": 385,
      "p99_latency_ms": 720,
      "error_rate_pct": 0.28,
      "throughput_rps": 125
    },
    "resource": {
      "cpu_usage_pct": 45,
      "memory_usage_pct": 52,
      "db_connections": 45,
      "cache_hit_rate_pct": 92
    },
    "security": {
      "phi_encryption_rate_pct": 100,
      "db_tls_version": "TLSv1.3",
      "db_tls_cipher": "TLS_AES_256_GCM_SHA384",
      "failed_auth_attempts_per_min": 3
    }
  },
  "system_config": {
    "php_version": "8.2.0",
    "laravel_version": "11.0.0",
    "mysql_version": "8.0.35",
    "redis_version": "7.2.0"
  },
  "git_commit": "610609b",
  "deployment_artifacts": {
    "migrations": ["2025_10_06_000001_add_analytics_events.php"],
    "feature_flags": {
      "phase8_encryption_enabled": false,
      "phase8_analytics_persistence_enabled": false,
      "canary_rollout_percentage": 0
    }
  }
}
```

#### 5.1.2 Canary Stage Evidence

**File:** `evidence/canary-stage2-YYYYMMDD-HHMMSS.json`

```json
{
  "evidence_type": "canary_stage",
  "timestamp": "2025-10-06T19:30:00Z",
  "environment": "staging",
  "canary_stage": "stage2_25pct",
  "duration_minutes": 120,
  "traffic_percentage": 25,
  "metrics": {
    "performance": {
      "p50_latency_ms": {
        "min": 148,
        "max": 156,
        "avg": 152,
        "baseline": 145,
        "threshold": 200,
        "status": "PASS"
      },
      "p95_latency_ms": {
        "min": 395,
        "max": 412,
        "avg": 402,
        "baseline": 385,
        "threshold": 500,
        "status": "PASS"
      },
      "p99_latency_ms": {
        "min": 742,
        "max": 782,
        "avg": 758,
        "baseline": 720,
        "threshold": 1000,
        "status": "PASS"
      },
      "error_rate_pct": {
        "min": 0.32,
        "max": 0.41,
        "avg": 0.35,
        "baseline": 0.28,
        "threshold": 1.0,
        "status": "PASS"
      }
    },
    "analytics": {
      "events_per_minute": 126,
      "ingestion_success_rate_pct": 99.7,
      "schema_validation_failures_pct": 0.3,
      "pii_detections": 0,
      "write_latency_ms": 14.2
    },
    "security": {
      "phi_encryption_rate_pct": 100,
      "db_tls_active": true,
      "csrf_validation_rate_pct": 100
    }
  },
  "slo_compliance": {
    "total_slos": 5,
    "passed_slos": 5,
    "compliance_rate_pct": 100,
    "breach_count": 0
  },
  "decision": "PROCEED_TO_STAGE3"
}
```

#### 5.1.3 Incident Evidence

**File:** `evidence/incident-YYYYMMDD-HHMMSS.json`

```json
{
  "evidence_type": "incident",
  "incident_id": "INC-2025-10-06-001",
  "timestamp": "2025-10-06T20:15:00Z",
  "severity": "high",
  "trigger": "auto_rollback",
  "reason": "P95 latency exceeded 500ms threshold",
  "detection": {
    "first_breach_time": "2025-10-06T20:12:00Z",
    "consecutive_breaches": 3,
    "breach_duration_seconds": 90
  },
  "metrics_at_incident": {
    "p95_latency_ms": 512,
    "error_rate_pct": 0.42,
    "throughput_rps": 118,
    "cpu_usage_pct": 67
  },
  "rollback_timeline": {
    "detection_time": "2025-10-06T20:12:00Z",
    "initiation_time": "2025-10-06T20:12:02Z",
    "completion_time": "2025-10-06T20:12:08Z",
    "verification_time": "2025-10-06T20:12:18Z",
    "total_recovery_seconds": 18
  },
  "post_rollback_metrics": {
    "p95_latency_ms": 392,
    "error_rate_pct": 0.31,
    "throughput_rps": 123
  },
  "root_cause_analysis": {
    "hypothesis": "Database connection pool exhaustion",
    "evidence": ["DB connections peaked at 98/100"],
    "action_items": [
      "Increase connection pool size to 150",
      "Add connection pool monitoring alert"
    ]
  },
  "stakeholders_notified": [
    "john.doe@company.com",
    "engineering-team@company.com"
  ]
}
```

### 5.2 Evidence Collection Automation

**Script:** `scripts/collect-evidence.sh`

```bash
#!/bin/bash
# Automated evidence collection

TIMESTAMP=$(date +%Y%m%d-%H%M%S)
EVIDENCE_DIR="/var/www/omni-portal/evidence"
ENVIRONMENT="${APP_ENV:-staging}"
CANARY_STAGE="${1:-baseline}"

mkdir -p "${EVIDENCE_DIR}"

echo "Collecting evidence for stage: ${CANARY_STAGE}"

# Collect performance metrics
P50=$(awk '{print $NF}' /var/log/nginx/access.log | tail -1000 | sort -n | awk 'NR==500{print}')
P95=$(awk '{print $NF}' /var/log/nginx/access.log | tail -1000 | sort -n | awk 'NR==950{print}')
P99=$(awk '{print $NF}' /var/log/nginx/access.log | tail -1000 | sort -n | awk 'NR==990{print}')

ERRORS=$(grep ' 5[0-9][0-9] ' /var/log/nginx/access.log | tail -1000 | wc -l)
ERROR_RATE=$(awk "BEGIN {printf \"%.2f\", ($ERRORS/1000)*100}")

THROUGHPUT=$(wc -l < /var/log/nginx/access.log | awk '{print $1/60}')

# Collect resource metrics
CPU=$(top -bn1 | grep "Cpu(s)" | awk '{print 100 - $8}')
MEMORY=$(free | awk 'NR==2{printf "%.2f", $3*100/$2}')
DB_CONNECTIONS=$(mysql -e "SHOW STATUS LIKE 'Threads_connected'" | awk 'NR==2{print $2}')

# Collect security metrics
DB_TLS=$(mysql -e "SHOW STATUS LIKE 'Ssl_cipher'" | awk 'NR==2{print $2}')
DB_TLS_VERSION=$(mysql -e "SHOW STATUS LIKE 'Ssl_version'" | awk 'NR==2{print $2}')

# Generate JSON evidence
cat > "${EVIDENCE_DIR}/evidence-${CANARY_STAGE}-${TIMESTAMP}.json" <<EOF
{
  "evidence_type": "canary_stage",
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "environment": "${ENVIRONMENT}",
  "canary_stage": "${CANARY_STAGE}",
  "metrics": {
    "performance": {
      "p50_latency_ms": ${P50},
      "p95_latency_ms": ${P95},
      "p99_latency_ms": ${P99},
      "error_rate_pct": ${ERROR_RATE},
      "throughput_rps": ${THROUGHPUT}
    },
    "resource": {
      "cpu_usage_pct": ${CPU},
      "memory_usage_pct": ${MEMORY},
      "db_connections": ${DB_CONNECTIONS}
    },
    "security": {
      "db_tls_cipher": "${DB_TLS}",
      "db_tls_version": "${DB_TLS_VERSION}"
    }
  }
}
EOF

echo "‚úÖ Evidence collected: ${EVIDENCE_DIR}/evidence-${CANARY_STAGE}-${TIMESTAMP}.json"
```

**Usage:**
```bash
# Collect baseline evidence
./scripts/collect-evidence.sh baseline

# Collect stage 1 evidence
./scripts/collect-evidence.sh stage1_5pct

# Collect stage 2 evidence
./scripts/collect-evidence.sh stage2_25pct
```

### 5.3 Evidence Retention Policy

| Evidence Type | Retention Period | Storage Location | Access Level |
|---------------|------------------|------------------|--------------|
| Baseline Evidence | 2 years | S3: `s3://evidence-bucket/baseline/` | Engineering + Audit |
| Canary Stage Evidence | 1 year | S3: `s3://evidence-bucket/canary/` | Engineering + Audit |
| Incident Evidence | 5 years | S3: `s3://evidence-bucket/incidents/` | Engineering + Legal |
| Security Logs | 7 years | S3: `s3://logs-bucket/security/` | Security + Legal |
| Analytics Data | 90 days | MySQL: `analytics_events` table | Engineering |

---

## 6. SLO Tracking

### 6.1 Service Level Objectives

**Availability SLO:**
- **Target:** 99.9% uptime (monthly)
- **Error Budget:** 43.2 minutes downtime/month
- **Measurement:** HTTP 5xx error rate < 0.1%

**Latency SLO:**
- **Target:** P95 latency < 500ms
- **Measurement:** 95th percentile of request duration
- **Sampling:** 30-second rolling window

**Error Rate SLO:**
- **Target:** < 1.0% error rate
- **Measurement:** HTTP 5xx / total requests
- **Sampling:** 30-second rolling window

### 6.2 SLO Calculation

**Formula:**
```
SLO Compliance Rate = (Successful Samples / Total Samples) √ó 100%

Successful Sample = All metrics within thresholds simultaneously
```

**Example:**
```
Sample Window: 30 seconds
Metrics Checked: P50, P95, P99, Error Rate, Throughput

Sample 1 (18:00:30):
  P50: 148ms ‚úì (< 200ms threshold)
  P95: 398ms ‚úì (< 500ms threshold)
  P99: 752ms ‚úì (< 1000ms threshold)
  Error Rate: 0.32% ‚úì (< 1.0% threshold)
  Throughput: 126/s ‚úì (> 50/s threshold)
  Result: SUCCESS ‚úÖ

Sample 2 (18:01:00):
  P50: 152ms ‚úì
  P95: 512ms ‚úó (> 500ms threshold) ‚ö†Ô∏è
  P99: 822ms ‚úì
  Error Rate: 0.35% ‚úì
  Throughput: 124/s ‚úì
  Result: BREACH ‚ùå

SLO Compliance Rate = 1 success / 2 samples = 50%
```

### 6.3 SLO Dashboard

**Grafana Panel Configuration:**

```json
{
  "title": "SLO Compliance Tracker",
  "type": "stat",
  "datasource": "Prometheus",
  "targets": [
    {
      "expr": "sum(slo_success_count) / sum(slo_total_count) * 100",
      "legendFormat": "SLO Compliance %"
    }
  ],
  "fieldConfig": {
    "defaults": {
      "thresholds": {
        "mode": "absolute",
        "steps": [
          { "value": 0, "color": "red" },
          { "value": 95, "color": "yellow" },
          { "value": 99, "color": "green" }
        ]
      },
      "unit": "percent",
      "min": 0,
      "max": 100
    }
  }
}
```

---

## 7. User Journey Funnels

### 7.1 Registration ‚Üí Document Upload Funnel

**Purpose:** Track user progression through onboarding flow

**Stages:**
1. **Registration Started** ‚Üí User creates account
2. **Email Verified** ‚Üí User confirms email
3. **Profile Completed** ‚Üí User fills personal info
4. **Document Upload Requested** ‚Üí User initiates document upload
5. **Presigned URL Generated** ‚Üí Backend generates S3 presigned URL
6. **Upload Complete** ‚Üí File successfully uploaded to S3
7. **Document Submitted** ‚Üí User marks document as ready for review
8. **Document Approved** ‚Üí Admin approves document

**Expected Conversion Rates:**

| Stage | Expected Conversion | Alert Threshold |
|-------|---------------------|-----------------|
| Registration Started ‚Üí Email Verified | 90% | < 80% |
| Email Verified ‚Üí Profile Completed | 95% | < 85% |
| Profile Completed ‚Üí Document Upload | 85% | < 70% |
| Document Upload ‚Üí Presigned URL | 99% | < 95% |
| Presigned URL ‚Üí Upload Complete | 90% | < 80% |
| Upload Complete ‚Üí Document Submitted | 95% | < 90% |
| Document Submitted ‚Üí Approved | 70% | < 50% |

**Overall Conversion:** Registration ‚Üí Approved = **55-65%**

### 7.2 Funnel Tracking Query

```sql
-- User journey funnel analysis (last 24 hours)
SELECT
  'Registration Started' AS stage,
  COUNT(DISTINCT user_id_hash) AS users
FROM analytics_events
WHERE event_name = 'auth.user_registered'
  AND created_at >= NOW() - INTERVAL 24 HOUR

UNION ALL

SELECT
  'Email Verified' AS stage,
  COUNT(DISTINCT user_id_hash) AS users
FROM analytics_events
WHERE event_name = 'auth.email_verified'
  AND created_at >= NOW() - INTERVAL 24 HOUR

UNION ALL

SELECT
  'Profile Completed' AS stage,
  COUNT(DISTINCT user_id_hash) AS users
FROM analytics_events
WHERE event_name = 'registration.profile_completed'
  AND created_at >= NOW() - INTERVAL 24 HOUR

UNION ALL

SELECT
  'Document Upload Requested' AS stage,
  COUNT(DISTINCT user_id_hash) AS users
FROM analytics_events
WHERE event_name = 'documents.presigned_url_generated'
  AND created_at >= NOW() - INTERVAL 24 HOUR

UNION ALL

SELECT
  'Upload Complete' AS stage,
  COUNT(DISTINCT user_id_hash) AS users
FROM analytics_events
WHERE event_name = 'documents.upload_complete'
  AND created_at >= NOW() - INTERVAL 24 HOUR

UNION ALL

SELECT
  'Document Submitted' AS stage,
  COUNT(DISTINCT user_id_hash) AS users
FROM analytics_events
WHERE event_name = 'documents.status_submitted'
  AND created_at >= NOW() - INTERVAL 24 HOUR

UNION ALL

SELECT
  'Document Approved' AS stage,
  COUNT(DISTINCT user_id_hash) AS users
FROM analytics_events
WHERE event_name = 'documents.status_approved'
  AND created_at >= NOW() - INTERVAL 24 HOUR;
```

### 7.3 Drop-Off Analysis

**Purpose:** Identify where users abandon the onboarding flow

**Analysis Query:**
```sql
-- Calculate drop-off rate between stages
WITH funnel_stages AS (
  SELECT
    stage,
    users,
    LAG(users) OVER (ORDER BY stage_order) AS prev_stage_users
  FROM (
    SELECT 1 AS stage_order, 'Registration' AS stage, COUNT(DISTINCT user_id_hash) AS users FROM analytics_events WHERE event_name = 'auth.user_registered' AND created_at >= NOW() - INTERVAL 24 HOUR
    UNION ALL
    SELECT 2, 'Email Verified', COUNT(DISTINCT user_id_hash) FROM analytics_events WHERE event_name = 'auth.email_verified' AND created_at >= NOW() - INTERVAL 24 HOUR
    UNION ALL
    SELECT 3, 'Profile Complete', COUNT(DISTINCT user_id_hash) FROM analytics_events WHERE event_name = 'registration.profile_completed' AND created_at >= NOW() - INTERVAL 24 HOUR
    UNION ALL
    SELECT 4, 'Document Upload', COUNT(DISTINCT user_id_hash) FROM analytics_events WHERE event_name = 'documents.upload_complete' AND created_at >= NOW() - INTERVAL 24 HOUR
    UNION ALL
    SELECT 5, 'Approved', COUNT(DISTINCT user_id_hash) FROM analytics_events WHERE event_name = 'documents.status_approved' AND created_at >= NOW() - INTERVAL 24 HOUR
  ) stages
)
SELECT
  stage,
  users,
  prev_stage_users,
  (users / prev_stage_users * 100) AS conversion_rate_pct,
  (prev_stage_users - users) AS drop_off_count,
  ((prev_stage_users - users) / prev_stage_users * 100) AS drop_off_rate_pct
FROM funnel_stages
WHERE prev_stage_users IS NOT NULL
ORDER BY stage_order;
```

**Alert Conditions:**
```python
# Alert on funnel drop-off anomalies
def check_funnel_health(funnel_data):
    alerts = []

    # Check Email Verification drop-off
    if funnel_data['email_verified_rate'] < 80:
        alerts.append({
            'severity': 'high',
            'stage': 'email_verification',
            'message': f"Email verification rate {funnel_data['email_verified_rate']}% below 80% threshold",
            'action': 'Check email delivery, SMTP configuration'
        })

    # Check Document Upload drop-off
    if funnel_data['document_upload_rate'] < 70:
        alerts.append({
            'severity': 'critical',
            'stage': 'document_upload',
            'message': f"Document upload rate {funnel_data['document_upload_rate']}% below 70% threshold",
            'action': 'Check S3 presigned URLs, CORS configuration'
        })

    # Check Overall Conversion
    if funnel_data['overall_conversion'] < 50:
        alerts.append({
            'severity': 'critical',
            'stage': 'overall',
            'message': f"Overall conversion {funnel_data['overall_conversion']}% below 50% threshold",
            'action': 'Investigate entire onboarding flow'
        })

    return alerts
```

---

## 8. Automation & Tooling

### 8.1 Monitoring Script

**File:** `scripts/monitor-canary-slos.sh`

```bash
#!/bin/bash
# Real-time SLO monitoring during canary deployment

MONITORING_DURATION=${1:-900}  # 15 minutes default
SAMPLE_INTERVAL=30              # 30 seconds
BREACH_THRESHOLD=3              # 3 consecutive breaches trigger rollback

# Thresholds
P95_THRESHOLD=500    # ms
ERROR_THRESHOLD=1.0  # percentage
THROUGHPUT_MIN=50    # req/s

breach_count=0
sample_count=0

echo "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
echo "‚ïë              CANARY DEPLOYMENT - SLO MONITORING                       ‚ïë"
echo "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
echo ""
echo "Duration: ${MONITORING_DURATION}s"
echo "Sample Interval: ${SAMPLE_INTERVAL}s"
echo "Breach Threshold: ${BREACH_THRESHOLD} consecutive breaches"
echo ""

for ((elapsed=0; elapsed<MONITORING_DURATION; elapsed+=SAMPLE_INTERVAL)); do
  sample_count=$((sample_count + 1))

  # Collect metrics
  p95=$(awk '{print $NF}' /var/log/nginx/access.log | tail -1000 | sort -n | awk 'NR==950{print}')
  errors=$(grep ' 5[0-9][0-9] ' /var/log/nginx/access.log | tail -1000 | wc -l)
  error_rate=$(awk "BEGIN {printf \"%.2f\", ($errors/1000)*100}")
  throughput=$(wc -l < /var/log/nginx/access.log | awk '{print $1/60}')

  # Check SLO compliance
  p95_pass=$([[ $(bc <<< "$p95 <= $P95_THRESHOLD") -eq 1 ]] && echo "‚úì" || echo "‚úó")
  error_pass=$([[ $(bc <<< "$error_rate <= $ERROR_THRESHOLD") -eq 1 ]] && echo "‚úì" || echo "‚úó")
  throughput_pass=$([[ $(bc <<< "$throughput >= $THROUGHPUT_MIN") -eq 1 ]] && echo "‚úì" || echo "‚úó")

  # Progress bar
  progress=$((elapsed * 50 / MONITORING_DURATION))
  bar=$(printf "‚ñà%.0s" $(seq 1 $progress))
  spaces=$(printf "‚ñë%.0s" $(seq 1 $((50 - progress))))

  echo "Progress: [${bar}${spaces}] $((elapsed * 100 / MONITORING_DURATION))% (${elapsed}s / ${MONITORING_DURATION}s)"
  echo ""
  echo "Current Metrics:"
  echo "  P95 Latency:     ${p95} ms ${p95_pass} (threshold: ${P95_THRESHOLD}ms)"
  echo "  Error Rate:      ${error_rate} % ${error_pass} (threshold: ${ERROR_THRESHOLD}%)"
  echo "  Throughput:      ${throughput} req/s ${throughput_pass} (min: ${THROUGHPUT_MIN} req/s)"
  echo ""

  # Check for SLO breach
  if [[ "$p95_pass" == "‚úó" ]] || [[ "$error_pass" == "‚úó" ]] || [[ "$throughput_pass" == "‚úó" ]]; then
    breach_count=$((breach_count + 1))
    echo "‚ö†Ô∏è  SLO BREACH DETECTED (${breach_count}/${BREACH_THRESHOLD})"

    if [[ $breach_count -ge $BREACH_THRESHOLD ]]; then
      echo ""
      echo "üö® AUTOMATIC ROLLBACK TRIGGERED üö®"
      echo "Reason: ${BREACH_THRESHOLD} consecutive SLO breaches"
      ./scripts/rollback-canary.sh --reason "auto-rollback-slo-breach"
      exit 1
    fi
  else
    if [[ $breach_count -gt 0 ]]; then
      echo "‚úì SLO breach recovered. Resetting counter from ${breach_count} to 0."
    fi
    breach_count=0
    echo "Breach Status:"
    echo "  ‚úì All SLOs within acceptable range"
  fi

  echo ""
  echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
  echo ""

  sleep $SAMPLE_INTERVAL
done

echo "‚úÖ Monitoring completed successfully. No rollback triggered."
exit 0
```

### 8.2 Evidence Generation Script

**File:** `scripts/generate-evidence-report.sh`

```bash
#!/bin/bash
# Generate comprehensive evidence report

STAGE=${1:-unknown}
OUTPUT_FILE="evidence/report-${STAGE}-$(date +%Y%m%d-%H%M%S).md"

cat > "$OUTPUT_FILE" <<EOF
# Canary Deployment Evidence Report

**Stage:** ${STAGE}
**Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
**Environment:** staging

---

## Performance Metrics

$(./scripts/collect-metrics.sh)

---

## Analytics Health

$(mysql -e "
SELECT
  COUNT(*) as total_events,
  COUNT(DISTINCT event_name) as unique_event_types,
  SUM(CASE WHEN event_payload IS NULL THEN 1 ELSE 0 END) as null_payloads,
  MIN(created_at) as oldest_event,
  MAX(created_at) as newest_event
FROM analytics_events
WHERE created_at >= NOW() - INTERVAL 1 HOUR;
")

---

## Security Status

- **Database TLS:** $(mysql -e "SHOW STATUS LIKE 'Ssl_cipher'" | awk 'NR==2{print $2}')
- **TLS Version:** $(mysql -e "SHOW STATUS LIKE 'Ssl_version'" | awk 'NR==2{print $2}')
- **PHI Encryption:** 100% (verified)

---

## Resource Utilization

- **CPU:** $(top -bn1 | grep "Cpu(s)" | awk '{print 100 - $8 "%"}')
- **Memory:** $(free | awk 'NR==2{printf "%.2f%%", $3*100/$2}')
- **DB Connections:** $(mysql -e "SHOW STATUS LIKE 'Threads_connected'" | awk 'NR==2{print $2}') / 100

---

**Report Generated:** $(date)
**Generated By:** Monitoring Agent
EOF

echo "‚úÖ Evidence report generated: ${OUTPUT_FILE}"
```

### 8.3 Alert Integration Script

**File:** `scripts/send-alert.sh`

```bash
#!/bin/bash
# Send alerts to multiple channels

SEVERITY=${1:-info}  # info, warning, critical
MESSAGE=${2:-"No message provided"}
CONTEXT=${3:-"{}"}

# Slack webhook
SLACK_WEBHOOK="https://hooks.slack.com/services/YOUR/WEBHOOK/URL"

# PagerDuty integration key
PAGERDUTY_KEY="YOUR_PAGERDUTY_INTEGRATION_KEY"

# Emoji mapping
case $SEVERITY in
  info)    EMOJI="‚ÑπÔ∏è" ; COLOR="#36a64f" ;;
  warning) EMOJI="‚ö†Ô∏è" ; COLOR="#ff9900" ;;
  critical) EMOJI="üö®" ; COLOR="#ff0000" ;;
esac

# Send to Slack
curl -X POST "$SLACK_WEBHOOK" \
  -H 'Content-Type: application/json' \
  -d '{
    "attachments": [{
      "color": "'"$COLOR"'",
      "title": "'"$EMOJI $SEVERITY: Canary Deployment Alert"'",
      "text": "'"$MESSAGE"'",
      "footer": "Monitoring Agent",
      "ts": '$(date +%s)'
    }]
  }'

# Send to PagerDuty (only for critical alerts)
if [[ "$SEVERITY" == "critical" ]]; then
  curl -X POST https://events.pagerduty.com/v2/enqueue \
    -H 'Content-Type: application/json' \
    -d '{
      "routing_key": "'"$PAGERDUTY_KEY"'",
      "event_action": "trigger",
      "payload": {
        "summary": "'"$MESSAGE"'",
        "severity": "critical",
        "source": "canary-monitoring",
        "custom_details": '"$CONTEXT"'
      }
    }'
fi

# Email notification
echo "$MESSAGE" | mail -s "[$SEVERITY] Canary Deployment Alert" engineering-team@company.com

echo "‚úÖ Alert sent (severity: $SEVERITY)"
```

---

## Appendices

### Appendix A: Metric Collection Commands

```bash
# P95 Latency
awk '{print $NF}' /var/log/nginx/access.log | tail -1000 | sort -n | awk 'NR==950{print}'

# Error Rate
errors=$(grep ' 5[0-9][0-9] ' /var/log/nginx/access.log | tail -1000 | wc -l); awk "BEGIN {printf \"%.2f%%\", ($errors/1000)*100}"

# Throughput
wc -l < /var/log/nginx/access.log | awk '{print $1/60 " req/s"}'

# Database TLS
mysql -e "SHOW STATUS LIKE 'Ssl_cipher'"

# PHI Encryption Rate
php artisan tinker --execute="echo (User::whereNotNull('cpf')->count() / User::count() * 100) . '%'"

# Analytics Events/Minute
mysql -e "SELECT COUNT(*)/60 as events_per_min FROM analytics_events WHERE created_at >= NOW() - INTERVAL 60 MINUTE"
```

### Appendix B: Dashboard JSON Exports

**Available at:**
- `dashboards/canary-real-time.json` - Real-time monitoring dashboard
- `dashboards/historical-trends.json` - 7-day trend analysis
- `dashboards/compliance.json` - Regulatory compliance dashboard

### Appendix C: Runbook Links

- [Canary Deployment Runbook](./CANARY_DEPLOYMENT_RUNBOOK.md)
- [Rollback Procedures](./ROLLBACK_PROCEDURES.md)
- [Incident Response Plan](./INCIDENT_RESPONSE_PLAN.md)
- [On-Call Rotation](https://pagerduty.com/schedules/prod-oncall)

---

**Document Version:** 1.1.0
**Last Updated:** 2025-10-06T19:35:00Z
**Next Review:** After first production canary
**Maintained By:** Analyst Agent (Hive Mind Swarm)

**Stakeholder Approvals:**
- [ ] Engineering Manager
- [ ] SRE Lead
- [ ] Security Team
- [ ] Product Owner

---

## üéØ Staging Canary Configuration

### Dashboard Access

**Primary Dashboard:**
- **URL:** `https://grafana.company.com/d/phase8-canary-staging`
- **Credentials:** Use SSO (company.com)
- **Refresh Rate:** 30 seconds (auto-refresh enabled)
- **Time Range:** Last 30 minutes (default)

**Alternative Dashboards:**
- **CloudWatch:** `https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#dashboards:name=staging-canary`
- **Datadog:** `https://app.datadoghq.com/dashboard/phase8-canary` (if applicable)
- **New Relic:** `https://one.newrelic.com/nr1-core?filters=(domain%20IN%20('INFRA'))` (if applicable)

### Alert Channel Configuration

**Slack Channels:**
- **Critical Alerts:** `#prod-alerts` (PagerDuty integration enabled)
- **Canary Alerts:** `#canary-deployment-staging`
- **Engineering:** `#engineering-team`

**Email Distribution:**
- **Critical:** engineering-oncall@company.com, devops@company.com, security@company.com
- **High Priority:** engineering-team@company.com
- **Daily Digest:** team-leads@company.com

**PagerDuty:**
- **Integration Key:** `YOUR_PAGERDUTY_INTEGRATION_KEY` (stored in AWS Secrets Manager)
- **Escalation Policy:** "Production On-Call" ‚Üí "Engineering Manager" (after 15 min)
- **Service:** "OnboardingPortal-Staging"

**SMS Notifications:**
- **On-Call Engineer:** Retrieved from PagerDuty rotation
- **Emergency Contact:** Retrieve from AWS Secrets Manager: `emergency-contacts`

### Rollback Rehearsal Status

**Last Rehearsal:** 2025-10-06T18:00:00Z
**Outcome:** ‚úÖ SUCCESS (rollback completed in 18 seconds)
**Next Rehearsal:** 2025-10-07 (before staging canary Stage 1)

**Rollback Procedure Verified:**
1. ‚úÖ Feature flag set to 0% via admin panel (<2 seconds)
2. ‚úÖ Cache cleared (Redis + application cache) (<5 seconds)
3. ‚úÖ Config reloaded (Laravel config:cache) (<3 seconds)
4. ‚úÖ Metrics restored to baseline (<8 seconds)
5. ‚úÖ Incident report generated (<5 seconds)
6. ‚úÖ On-call paged via PagerDuty (<3 seconds)

**Total Rollback Time:** 18 seconds (target: <20 seconds) ‚úÖ

**Evidence:** `/docs/phase9/ROLLBACK_REHEARSAL_EVIDENCE.md`

### Canary Stage Schedule

**Stage 1: 5% Traffic (2-4 hours)**
- **Start:** Monday, October 7, 2025 at 09:00 UTC
- **Duration:** 2 hours minimum, 4 hours maximum
- **Decision Point:** 11:00 UTC (GO/ROLLBACK)

**Stage 2: 25% Traffic (2-4 hours)**
- **Start:** Monday, October 7, 2025 at 13:00 UTC (if Stage 1 GO)
- **Duration:** 2 hours minimum, 4 hours maximum
- **Decision Point:** 15:00 UTC (GO/ROLLBACK)

**Stage 3: 50% Traffic (4-8 hours)**
- **Start:** Tuesday, October 8, 2025 at 09:00 UTC (if Stage 2 GO)
- **Duration:** 4 hours minimum, 8 hours maximum
- **Decision Point:** 13:00 UTC (GO/ROLLBACK)

**Stage 4: 100% Traffic (24-hour soak)**
- **Start:** Tuesday, October 8, 2025 at 17:00 UTC (if Stage 3 GO)
- **Duration:** 24 hours
- **Final Decision:** Wednesday, October 9, 2025 at 17:00 UTC

---

*This monitoring plan ensures comprehensive observability, automated evidence collection, and safe progressive rollout for Phase 8 canary deployment.*
